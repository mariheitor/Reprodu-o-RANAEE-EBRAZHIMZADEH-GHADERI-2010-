import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score
from scipy.stats import skew, kurtosis
import random
import urllib.request

# Baixar os dados
url = "https://kdd.ics.uci.edu/databases/synthetic_control/synthetic_control.data"
file_path = "synthetic_control.data"
urllib.request.urlretrieve(url, file_path)

# Carregar os dados
def load_data(file_path):
    data = np.loadtxt(file_path)
    labels = np.repeat(np.arange(6), 100)  # 6 classes, 100 instâncias cada
    return data, labels

# Extração de características
def extract_features(data):
    features = []
    for series in data:
        feature_vector = [
            np.mean(series),
            np.std(series),
            skew(series),
            kurtosis(series),
            np.polyfit(range(len(series)), series, 1)[0],  # Slope (S)
            np.sum(np.diff(np.sign(series - np.mean(series)))) // 2,  # NC1
            np.sum(np.diff(np.sign(series - np.polyfit(range(len(series)), series, 1)[1]))) // 2,  # NC2
            np.mean(np.abs(np.diff(series))),  # AS
            np.abs(np.polyfit(range(len(series)), series, 1)[0] - np.mean(np.abs(np.diff(series)))),  # SD
            np.sum(np.abs(series - np.mean(series))),  # APML
            np.sum(np.abs(series - np.polyfit(range(len(series)), series, 1)[1])),  # APSL
            np.sum(np.abs(np.polyfit(range(len(series)), series, 1)[1] - np.mean(np.abs(np.diff(series))))),  # ASS
        ]
        features.append(feature_vector)
    return np.array(features)

# Função para otimizar os parâmetros com PSO
def objective_function(params, X_train, y_train, X_test, y_test):
    C, gamma = params
    model = SVC(C=C, kernel='rbf', gamma=gamma, decision_function_shape="ovr")
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    return -accuracy_score(y_test, y_pred)

# PSO para otimizar apenas C e gamma
def pso_optimization(objective_function, lb, ub, args, swarm_size=20, max_iter=100):
    particles = np.random.uniform(low=lb, high=ub, size=(swarm_size, len(lb)))
    velocities = np.zeros_like(particles)

    personal_best_positions = particles.copy()
    personal_best_scores = np.array([objective_function(p, *args) for p in particles])
    global_best_position = personal_best_positions[np.argmin(personal_best_scores)]

    w, c1, c2 = 3, 2, 2  # Hiperparâmetros do PSO

    for _ in range(max_iter):
        for i in range(swarm_size):
            velocities[i] = (w * velocities[i] +
                             c1 * random.random() * (personal_best_positions[i] - particles[i]) +
                             c2 * random.random() * (global_best_position - particles[i]))
            particles[i] += velocities[i]
            particles[i] = np.clip(particles[i], lb, ub)

            score = objective_function(particles[i], *args)

            if score < personal_best_scores[i]:  # Minimizar erro (-acurácia)
                personal_best_scores[i] = score
                personal_best_positions[i] = particles[i]

        global_best_position = personal_best_positions[np.argmin(personal_best_scores)]

    return global_best_position[0], global_best_position[1]

# Nomes reais dos padrões no dataset
pattern_names = [
    "Normal",
    "IT",
    "DT",
    "CYC",
    "US",
    "DS"
]


# Gráfico 3D dos clusters
def plot_3d_clusters(X, y, feature_indices, feature_names):
    fig = plt.figure(figsize=(10, 7))
    ax = fig.add_subplot(111, projection='3d')

    colors = ['r', 'g', 'b', 'c', 'm', 'y']
    markers = ['o', '^', 's', 'P', 'D', '*']

    for i in range(6):  # 6 classes
        indices = np.where(y == i)
        ax.scatter(X[indices, feature_indices[0]], X[indices, feature_indices[1]], X[indices, feature_indices[2]],
                   c=colors[i], marker=markers[i], label=pattern_names[i])

    ax.set_xlabel(feature_names[feature_indices[0]])
    ax.set_ylabel(feature_names[feature_indices[1]])
    ax.set_zlabel(feature_names[feature_indices[2]])
    ax.set_title('Visualização 3D dos clusters de classes')
    ax.legend()
    plt.show()

if __name__ == "__main__":
    # Carregar e processar dados
    data, labels = load_data(file_path)
    X = extract_features(data)
    y = labels

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # Definição dos hiperparâmetros e das features
    feature_names = ["Mean", "Std Dev", "Skewness", "Kurtosis", "Slope (S)", "NC1", "NC2", "AS", "SD", "APML", "APSL", "ASS"]
    lb = [1, 0]  # Limites inferiores para C e gamma
    ub = [1000, 1.0]  # Limites superiores para C e gamma

    # Rodar PSO para encontrar os melhores parâmetros
    best_C, best_gamma = pso_optimization(objective_function, lb, ub,
                                          args=(X_train, y_train, X_test, y_test))

    # Treinar o modelo final com os parâmetros otimizados
    final_model = SVC(C=best_C, kernel='rbf', gamma=best_gamma, decision_function_shape="ovo")
    final_model.fit(X_train, y_train)
    y_pred = final_model.predict(X_test)

    print(f"Melhores parâmetros: C={best_C}, Gamma={best_gamma}")
    print(f"Acurácia final: {accuracy_score(y_test, y_pred):.4f}")

    # Escolher manualmente as features para o gráfico
    chosen_features = [0, 9, 11]  # Índices das features desejadas
    plot_3d_clusters(X, y, chosen_features, feature_names)
